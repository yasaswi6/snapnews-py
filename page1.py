import streamlit as st
from PIL import Image
from bs4 import BeautifulSoup as soup
from urllib.request import urlopen, Request
from newspaper import Article
import io
from gtts import gTTS
import base64
import random
import requests
import re
import os
import pandas as pd
import csv
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

nltk.download('punkt')
nltk.download('stopwords')

if 'saved_articles' not in st.session_state:
    st.session_state['saved_articles'] = []

if 'saved_status' not in st.session_state:
    st.session_state['saved_status'] = {}

if 'page_number' not in st.session_state:
    st.session_state['page_number'] = 0

if 'search_page_number' not in st.session_state:
    st.session_state['search_page_number'] = 0

NEWS_API_KEY = 'ec48b2493593467a8947d0253d2786a2'
COMMENTS_CSV = 'comments.csv'
USERS_CSV = 'users.csv'

def fetch_rss_feed(url):
    try:
        op = urlopen(Request(url, headers={'User-Agent': 'Mozilla/5.0'}))
        rd = op.read()
        op.close()
        sp_page = soup(rd, 'xml')
        news_list = sp_page.find_all('item')
        return news_list
    except Exception as e:
        st.error(f"Error fetching news: {e}")
        return []

def fetch_news_poster(poster_link):
    try:
        u = urlopen(Request(poster_link, headers={'User-Agent': 'Mozilla/5.0'}))
        raw_data = u.read()
        image = Image.open(io.BytesIO(raw_data))
        st.image(image, use_column_width=True)
    except Exception as e:
        st.error(f"Error fetching image: {e}")
        image = Image.open('snap.png')
        st.image(image, use_column_width=True)

def save_article(index, title, link, summary):
    st.session_state['saved_articles'].append({'title': title, 'link': link, 'summary': summary})
    st.session_state['saved_status'][index] = True
    st.success(f'Article "{title}" saved!')

def unsave_article(index, title):
    st.session_state['saved_articles'] = [article for article in st.session_state['saved_articles'] if article['title'] != title]
    st.session_state['saved_status'][index] = False
    st.success(f'Article "{title}" removed!')

def load_saved_articles():
    st.subheader("Saved Articles")
    for index, article in enumerate(st.session_state['saved_articles']):
        st.write(f"**{article['title']}**")
        st.write(f"[Read more...]({article['link']})")
        st.write(f"{article['summary']}")
        if st.button("Unsave", key=f"unsave_{index}_saved"):
            unsave_article(index, article['title'])
        st.write("---")

def text_to_speech(text, lang='en'):
    tts = gTTS(text=text, lang=lang)
    audio_file = f"audio_{text[:10].replace(' ', '_')}.mp3"
    tts.save(audio_file)
    audio_data = open(audio_file, "rb").read()
    b64 = base64.b64encode(audio_data).decode()
    audio_html = f"""
    <audio controls>
        <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
        Your browser does not support the audio element.
    </audio>
    """
    return audio_html

def extract_article_text(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text, article.top_image
    except Exception as e:
        st.error(f"Newspaper library failed to extract article: {e}")
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            req = Request(url, headers=headers)
            page = urlopen(req).read()
            page_soup = soup(page, 'html.parser')
            paragraphs = page_soup.find_all('p')
            text = ' '.join([para.text for para in paragraphs])
            top_image = page_soup.find('meta', property='og:image')
            top_image = top_image['content'] if top_image else 'snap.png'
            return text, top_image
        except Exception as e:
            st.error(f"BeautifulSoup failed to extract article: {e}")
            return None, 'snap.png'

def summarize_text(text):
    stop_words = set(stopwords.words("english"))
    words = word_tokenize(text)

    freq_table = dict()
    for word in words:
        word = word.lower()
        if word in stop_words:
            continue
        if word in freq_table:
            freq_table[word] += 1
        else:
            freq_table[word] = 1

    sentences = sent_tokenize(text)
    sentence_value = dict()

    for sentence in sentences:
        for word, freq in freq_table.items():
            if word in sentence.lower():
                if sentence in sentence_value:
                    sentence_value[sentence] += freq
                else:
                    sentence_value[sentence] = freq

    sum_values = 0
    for sentence in sentence_value:
        sum_values += sentence_value[sentence]

    average = int(sum_values / len(sentence_value))

    summary = ''
    for sentence in sentences:
        if sentence in sentence_value and sentence_value[sentence] > (1.5 * average):
            summary += " " + sentence
    return summary

def display_news(list_of_news, page_number, language, s):
    from googletrans import Translator
    translator = Translator()
    items_per_page = 5
    start_index = page_number * items_per_page
    end_index = start_index + items_per_page
    news_to_display = list_of_news[start_index:end_index]

    for i, news in enumerate(news_to_display):
        index = start_index + i
        title = news.title.text if news.title else "No title"
        link = news.link.text if news.link else "No link"
        source_tag = news.source if news.source else None
        source = "Unknown source" if source_tag is None else source_tag.text.strip()
        
        st.write(f'**({index + 1}) {title}**')
        if not link or not link.startswith('http'):
            st.warning(f"Skipping article with invalid URL: {link}")
            continue

        try:
            article_text, top_image = extract_article_text(link)
            if not article_text:
                continue
            summary = summarize_text(article_text)
            summary_translated = translator.translate(summary, dest=language).text
        except Exception as e:
            st.error(f"Error processing article {link}: {e}")
            continue

        fetch_news_poster(top_image)

        with st.expander(title):
            st.markdown(f"<h6 style='text-align: justify;'>{summary_translated}</h6>", unsafe_allow_html=True)
            st.markdown(f"[Read more at {source}]({link})")
            audio_html = text_to_speech(summary_translated)
            st.markdown(audio_html, unsafe_allow_html=True)

            if st.session_state['saved_status'].get(index, False):
                if st.button("Unsave", key=f"unsave_{index}"):
                    unsave_article(index, title)
            else:
                if st.button("Save", key=f"save_{index}"):
                    save_article(index, title, link, summary_translated)

            st.write("---")
            st.write("Share on:")
            st.markdown(
                f"""
                <a href="https://www.facebook.com/sharer/sharer.php?u={link}" target="_blank">
                <img src="https://img.icons8.com/fluent/48/000000/facebook-new.png"/>
                </a>
                <a href="https://twitter.com/intent/tweet?url={link}&text={title}" target="_blank">
                <img src="https://img.icons8.com/fluent/48/000000/twitter.png"/>
                </a>
                <a href="https://www.linkedin.com/shareArticle?mini=true&url={link}&title={title}" target="_blank">
                <img src="https://img.icons8.com/fluent/48/000000/linkedin.png"/>
                </a>
                """,
                unsafe_allow_html=True
            )
            st.success("Published Date: " + news.pubDate.text)

            # Display and add comments
            st.write("Comments:")
            comments = load_comments(link)
            for comment in comments:
                st.write(f"{comment['username']}: {comment['comment']}")
            new_comment = st.text_area(f"Add a comment for article {index + 1}", key=f"comment_{index}")
            if st.button("Submit", key=f"submit_{index}"):
                add_comment(link, new_comment, s)

def display_search_news(list_of_news, page_number):
    items_per_page = 5
    start_index = page_number * items_per_page
    end_index = start_index + items_per_page
    news_to_display = list_of_news[start_index:end_index]

    for i, news in enumerate(news_to_display):
        index = start_index + i
        title = news.title.text if news.title else "No title"
        link = news.link.text if news.link else "No link"
        source_tag = news.find('source')
        source = "Unknown source" if source_tag is None else source_tag.text.strip()

        st.markdown(f"### {index + 1}. {title}")
        st.markdown(f"*Source: {source}*")

        try:
            article_text, top_image = extract_article_text(link)
            if not article_text:
                continue
            summary = summarize_text(article_text)
            st.markdown(f"<p>{summary}</p>", unsafe_allow_html=True)
        except Exception as e:
            st.error(f"Error processing article {link}: {e}")
            continue

        st.markdown(f"[Read more at {source}]({link})")
        st.write("---")

    col1, col2, col3 = st.columns([1, 1, 1])

    if page_number > 0:
        with col1:
            if st.button("Previous", key="prev_search"):
                st.session_state['search_page_number'] -= 1
                st.experimental_rerun()

    if end_index < len(list_of_news):
        with col3:
            if st.button("Next", key="next_search"):
                st.session_state['search_page_number'] += 1
                st.experimental_rerun()

def fetch_real_breaking_news():
    url = f'https://newsapi.org/v2/top-headlines?country=us&apiKey={NEWS_API_KEY}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        if data['status'] == 'ok' and data['totalResults'] > 0:
            return random.choice(data['articles'])['title']
    return "No breaking news available at the moment."

def simulate_notifications():
    notification = fetch_real_breaking_news()
    st.sidebar.info(notification)

def remove_emojis(input_string):
    return re.sub(r'[^\w\s,]', '', input_string)

def add_comment(article_url, comment, username="Anonymous"):
    try:
        if comment.strip() == "":
            st.error("Comment cannot be empty")
            return

        new_comment = {"article_url": article_url, "comment": comment, "username": username}

        # Read existing comments
        if os.path.exists(COMMENTS_CSV):
            with open(COMMENTS_CSV, mode='r', newline='', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                comments = list(reader)
        else:
            comments = []

        # Add new comment
        comments.append(new_comment)

        # Write updated comments back to the CSV
        with open(COMMENTS_CSV, mode='w', newline='', encoding='utf-8') as file:
            fieldnames = ["article_url", "comment", "username"]
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(comments)

        st.success("Comment added successfully!")

    except Exception as e:
        st.error(f"Error saving comment: {e}")

def load_comments(article_url):
    try:
        if os.path.exists(COMMENTS_CSV):
            comments_df = pd.read_csv(COMMENTS_CSV)
            article_comments = comments_df[comments_df['article_url'] == article_url]
            return article_comments.to_dict('records')
        else:
            return []
    except Exception as e:
        st.error(f"Error loading comments: {e}")
        return []

def main():
    if 'saved_articles' not in st.session_state:
        st.session_state['saved_articles'] = []

    if 'saved_status' not in st.session_state:
        st.session_state['saved_status'] = {}

    if 'page_number' not in st.session_state:
        st.session_state['page_number'] = 0

    if 'search_page_number' not in st.session_state:
        st.session_state['search_page_number'] = 0

    st.markdown("<h1 style='text-align: center;'>SnapNewsüá∏üá¨: News Anytime, Anywhere üåçüïí</h1>", unsafe_allow_html=True)
    image = Image.open('snap.png')

    col1, col2, col3 = st.columns([3, 5, 3])

    with col1:
        st.write("")

    with col2:
        st.image(image, use_column_width=False)

    with col3:
        st.write("")

    if st.sidebar.button("Simulate Notification"):
        simulate_notifications()

    category = ['--Select--', 'üî• Hot News', 'üíô Top Picks', 'üîç Explore']
    cat_op = st.selectbox('Choose Your News', category)

    news_list = []

    if cat_op == category[0]:
        st.warning('Please select a category!')
    elif cat_op == category[1]:
        st.subheader("üî• Hot News")
        news_list = fetch_rss_feed('https://www.yahoo.com/news/rss')
        display_news(news_list, st.session_state['page_number'], 'en', st.session_state['username'])
    elif cat_op == category[2]:
        av_topics = ['Choose Topic', 'üíº Business', 'üíª Tech', '‚öñÔ∏è Politics', 'üåç World', '‚öΩ Sports']
        st.subheader("üíô Top Picks")
        chosen_topic = st.selectbox("Choose your favourite topic", av_topics)
        if chosen_topic == av_topics[0]:
            st.warning("Please choose a topic")
        else:
            if chosen_topic == 'üíº Business':
                news_list = fetch_rss_feed('https://finance.yahoo.com/rss/')
            elif chosen_topic == 'üíª Tech':
                news_list = fetch_rss_feed('https://www.yahoo.com/news/tech/rss')
            elif chosen_topic == '‚öñÔ∏è Politics':
                news_list = fetch_rss_feed('https://news.yahoo.com/rss/politics')
            elif chosen_topic == 'üåç World':
                news_list = fetch_rss_feed('https://news.yahoo.com/rss/world')
            elif chosen_topic == '‚öΩ Sports':
                news_list = fetch_rss_feed('https://sports.yahoo.com/rss/')
            
            if news_list:
                st.subheader(f"üíô Here are some {chosen_topic.split()[-1]} news for you")
                display_news(news_list, st.session_state['page_number'], 'en', st.session_state['username'])
            else:
                st.error(f"No news found for {chosen_topic}")

    elif cat_op == category[3]:
        user_topic = st.text_input("Enter Your Topicüîç")

        if st.button("Search") and user_topic != '':
            user_topic_pr = remove_emojis(user_topic.replace(' ', ''))
            news_list = fetch_rss_feed(f"https://news.google.com/rss/search?q={user_topic_pr}&hl=en-IN&gl=IN&ceid=IN:en")
            if news_list:
                st.subheader(f"üîç Here are some {user_topic.capitalize()} news for you")
                display_search_news(news_list, st.session_state['search_page_number'])
            else:
                st.error(f"No news found for {user_topic}")
        else:
            st.warning("Please enter a topic name to searchüîç")

    load_saved_articles()

if __name__ == "__main__":
    main()